#deck:"Probability & Statistics"
#noteypte:"basic-bf4b5"
Variance of random variable X,\(\mathbb{E}[(X-\mathbb{E}[x])^2]\)
Standard deviation of random variable X,\(\sqrt{Var[X]}\)
Epistemic uncertainty,Ignorance with respect to underlying data-generating process; model uncertainty
Alaetoric uncertainty,Intrisic variability; data uncertainty
Mode of a distribution,\(argmax_x pdf(X=x)\)
Data missing completely at random,\( M \perp\!\!\!\perp X\)

Data missing at random,"\( M \perp\!\!\!\perp o(X; m) | p(X; m)\) where
- \(m\) is a mask,
- \(X\) all true values and
- \(o\) filters only observables"

Data not missing at random,The missing value is correlated with the fact that it is missing
Data missing due to censorship,The fact that a value is missing is deterministically dependent on its underlying value
Dirac-delta function,\(\delta(x) = \begin{cases} + \infty & \text{ if }\ x=0 \\ 0 & \text{ otherwise} \end{cases}\) where \( \int_{-\infty}^{\infty} \delta(x) dx = 1\)
Homoscedasticity,A.k.a homogeneity of variances; Rule of thumb: if the ratio of the largest variance to the smallest variance is 1.5 or below the data is homoscedastic.
Heteroscedasticity,The variance of a variable is unequal across the range of values of a second variable that predicts it.
Pdf of a univariate Normal,\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} exp(-\frac{1}{2}(\frac{x - \mu}{\sigma})^2) \) where \(\mu\) is the mean and \(\sigma\) the standard deviation
Covariance of \(X\) and \(Y\),Measures degree to which \(X\) and \(Y\) are linearly related. \(\mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] \)
Pearson correlation coefficient,'Normalized covariance' - between -1  and 1. \(\rho = corr[X; Y] = \frac{Cov[X; Y]}{\sqrt{Var[X]Var[Y]}}\)

Justification for Maximum Likelihood Estimation,"1. Maximum A Posteriori with uniform prior
2.Minimal KL divergence to empirical distribution of data"

Ordinary Least Squares estimate,\(\hat{w}_{MLE} = argmin\ RSS(w) = (X^TX)^{-1}X^Ty\)
Exponentially weighted moving average,\(\hat{\mu}_t = \beta \hat{\mu}_{t-1} + (1-\beta) y_t\)
Cross-validated risk,\( R_{\lambda}^{CV} = \frac{1}{K} \sum_{k=1}^K R(\hat{\theta}_{\lambda} (\mathcal{D}_{-k}); \mathcal{D}_k)\)
Standard error of a statistic,Standard deviation of the sampling distribution
Conjugate prior,A prior is called conjugate to a likelihood if the posterior is in the same family as the prior.
Posterior predictive distribution,\( p(y|x;D) = \int p(y|x; \theta) p(\theta|\mathcal{D}) d\theta\)
Empirical Bayes,Aka type II maximum likelihood; \(argmax_{\psi} \int p(\mathcal{D}|\theta)p(\theta|\psi)d\theta\)
\(100(1-\alpha) \)% Credible interval,Quantifies uncertainty associated with estimate; \(C_{\alpha}(\mathcal{D}) = (l; u): P(l \leq \theta \leq u | \mathcal{D}) = 1 - \alpha \)
\(100(1-\alpha) \)% Central interval,Credible interval with \((1-\alpha)/2\) mass in each tail
Highest density interval,\(C_{\alpha}(\mathcal{D}) = \{\theta: p(\theta|\mathcal{D}) \geq p^*\} \) where \(p^*\) is determined such that \(1 - \alpha = \int_{\theta: p(\theta|\mathcal{D}) > p^*} p(\theta|\mathcal{D}) d\theta\)
Parametric bootstrap,Create \(S\) many datasets \(\mathcal{D}^{(s)} = \{x_n \sim p(x_n|\hat{\theta}): n=1:N\} \) and use them to estimate \(p(\pi(\mathcal{D}) = \theta|\mathcal{D} \sim \theta^*) = \frac{1}{S}\sum_{s=1}^S \delta(\theta = \pi(\mathcal{D}^{(s)})) \)

Non-parametric bootstrap,Create \(S\) many datasets by sampling with replacement \(\mathcal{D}^{(s)} = \{x_n \sim \mathcal{D}: n=1:N\} \) and use them to estimate \(p(\pi(\mathcal{D}) = \theta|\mathcal{D} \sim \theta^*) = \frac{1}{S}\sum_{s=1}^S \delta(\theta = \pi(\mathcal{D}^{(s)})) \)

\(100(1-\alpha) \)% Confidence interval,\(I(\mathcal{D}) = (l(\mathcal{D}); u(\mathcal{D})) \) such that \(\Pr[\theta \in I(\mathcal{D})|\mathcal{D} \sim \theta] = 1 - \alpha\)

Confidence interval vs Credible interval,"Credible interval: What's the probability that the underlying parameter stems from a given interval given the data?
Confidence interval: What's the probability that the data confirms the parameter?
Bayesian: parameter is random and data is fixed.
Frequentist: parameter is fixed data is random."

Bias-variance tradeoff,\(MSE = variance + bias^2\)

Sufficient statistic \(t = T(X)\) for \(\theta\),"1. \(t\) is sufficient for underlying parameter \(\theta\) precisely if the conditional probability distribution of the data \(X\), given the statistic \(t = T(X)\), does not depend on the parameter \(\theta\).
2. \(I(\theta; T(X)) = I(\theta, X)\) where \(I\) is the mutual information."

Consistent estimator,"Formally speaking, an estimator \(T_n\) of parameter \(\theta\) is said to be consistent, if it converges in probability to the true value of the parameter."

Bias of an estimator,"is the difference between this estimator's expected value and the true value of the parameter being estimated."

Example of a biased but consistent estimator,"Assume \(X_i \sim \mathcal{U}(0, \theta)\). We define our estimator for \(\theta\) as follows: \(\hat{\theta} = \max_i X_i\). Intuitively this is biased because it is always, in expectation, underestimating the true parameter. Formally, its expected value is \(\mathbb{E}[\hat{\theta}] = \frac{n}{n+1}\theta\)."

Example of an unbiased but inconsistent estimator,"Given a series of \(n\) numbers, we would like to estimate the true underlying mean.
We can define the estimator to be the most recently drawn number."

Example of a simple Bayesian but non-generative model,"Linear regression where the \(X_i\) are considered constants and the weights \(w\) are considered random variables."

Weak Law of Large Numbers,"Given an infinite series of Lebesque integrable i.i.d. variables \(X_1, X_2, \dots\) for which it holds that \(\mathbb[E][X_i] = \mu\).
We define the sample average as \(\bar{X}_n = \frac{1}{n}\sum_i X_i\).
Then we know that the sample average converges in probability towards the expected value, i.e. \(\forall \epsilon > 0\):
\(\lim_{n \rightarrow \infty} \Pr(|\bar{X}_n - \mu| < \epsilon) = 1\)"

Strong Law of Large Numbers,"A.k.a. Kolmogorov's law. Given an infinite series of Lebesque integrable i.i.d. variables \(X_1, X_2, \dots\) for which it holds that \(\mathbb[E][X_i] = \mu\).
We define the sample average as \(\bar{X}_n = \frac{1}{n}\sum_i X_i\).
Then we know that the sample average converges almost surely to the expected value, i.e.:
\(\Pr(\lim_{n \rightarrow \infty} \bar{X}_n = \mu) = 1\)"

Classic Central Limit Theorem,"Assume \(X_1, \dots, X_n\) are a sequence of i.i.d random variables such that \(\mathbb{E}[X_i] = \mu\) and \(Var[X_i] = \sigma^2\).
The LLN tells us that the sample mean converges almost surely to \(\mu\). The CCLT tells us that as \(n\) grows, the distribution of the difference between the sample average \(\hat{X}_n\)
and \(\mu\) multiplied by \(\sqrt{n}\) approximates a normal distribution with mean 0 and variance \(\sigma^2\), i.e.:
\(\sqrt{n} (\hat{X}_n - \mu) \rightarrow \mathcal{N}(0, \sigma^2)\)"

Skewness,"Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable around its mean. For a unimodal distribution,
negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right.
Skewness is the third standardized moment of a distribution.
\(\gamma_1 = \tilde{\mu}_3 = \mathbb{E}[(\frac{X - \mu}{\sigma})^3] \)
"