Reinforcement Learning vs Supervised Learning, Learning with a critic vs learning with a teacher
\(TFIDF_{ij} \) term \(i \) document \(j\), \( \frac{log(TF_{ij} + 1)}{log(\frac{n_{docs}}{n_{docs \text{ with term } i}})}\)
Logistic function; a sigmoid function, \(\frac{1}{1+e^{-x}}\)
Sofmax of vector \(a\), \(S(a) = [\frac{e^{a_1}}{\sum_{c=1}^C e^{a_c}}  \dots \frac{e^{a_C}}{\sum_{c=1}^C e^{a_c}} ]\) where \(a\) is called logits or generalized log odds
Logit, \(\sigma^{-1}(p) = \log(\frac{p}{1-p})\)

Weak supervision, Using metadata as labels for data; e.g. hashtags associated with a picture
Paradigms of self-supervision in CV, "1. Invariance (function of input should equal function of perturbed input)
2. Pretext tasks (predicting properties)"

Invariance, \(f(g(x)) = f(x)\)
Equivariance, \(f(g(x)) = g(f(x))\)
Topology (informal), Geometric notion of neighborhood without notion of distance
Manifold (informal), Topological space which resembles a Euclidean space locally; near each point.

LIME explanation, "\( \xi(x) = argmin_{g' \in G} \mathcal{L}(f, g', \pi_x) + \Omega(g')\) with
- \(f\), the original model
- \(G\) the class of possible, interpretable surrogate models
- \(\Omega(g)\), a measure of complexity for \(g \in G\)
- \(\pi_x(z)\) a proximity measure of \(z\) wrt data point \(x\)
- \(\mathcal{L}(f, g, \pi_x)\) a measure of how unfaithful a \(g \in G\) is to \(f\) in the locality defined by \(\pi_x\)
"

Shapley value, "\( \psi_i(v) = \sum_{S \subset N \setminus \{i\}} {|N| \choose 1, |S|, n-|S|-1}^{-1} v(S \cup \{i\}) - v(S) \)
for player \(i \in N\) wrt payoff \(v\)
"

Kinds of self-supervision, "1. Pretext task
2. Invariance"

Entropy of a discrete random variable \(X\) with distribution \(p\), \(\mathbb{H}(X) = \mathbb{H}(p) = -\sum_{x \in \mathcal{X}}p(x)\log(p(x)) = \mathbb{E}[-\log(p(x))]\)
Cross-entropy between discrete distributions \(p\) and \(q\) with support \(\mathcal{X}\), "\(\mathbb{H}(p, q) = -\sum_{x \in \mathcal{X}} p(x) \log q(x)\)"
Joint entropy, "\(\mathbb{H}(X, Y) := -\sum_{\mathcal{X}, \mathcal{Y}} p(X=x, Y=y)\log_2 p(X=x, Y=y)\) "
Conditional entropy, "\(\begin{aligned} \mathbb{H}(X|Y) &:= \mathbb{E}_{p(X)}[\mathbb{H}(p(Y|X))] \\ &= \mathbb{H}(X, Y) - \mathbb{H}(X) \end{aligned}\)"
KL-divergence between \(p\) and \(q\), "\( D_{KL}(p\|\|q) := \sum_{\mathcal{X}} p(X=x) \log_2 \frac{p(X=x)}{q(X=X)} = \mathbb{H}(p, q) - \mathbb{H}(p) \)"
Forwards KL-divergence, Approximating \(p\) with \(q\) by minimizing \(D_{KL}(p\|q)\) w.r.t. \(q\)
Backwards KL-divergence, Approximating \(p\) with \(q\) by minimizing \(D_{KL}(q\|p)\) w.r.t. \(q\)
(Expected) Mutual Information a.k.a. Information Gain, "\(\begin{aligned} \mathbb{I}(X;Y) &:= D_{KL}(p(X, Y) \| p(X)p(Y)) \\ &= \mathbb{H}(X) - \mathbb{H}(X\|Y) \\ &= \mathbb{H}(X) + \mathbb{H}(Y) - \mathbb{H}(X, Y) \end{aligned}\)"
Conditional Mutual Information, "\(\begin{aligned} \mathbb{I}(X;Y|Z) &:= \mathbb{E}_{p(Z)} [ \mathbb{I}(X;Y) | Z] \\ &= \mathbb{I}(Y; X, Z) - \mathbb{I}(Y;Z) \end{aligned} \)"

Interpretation of entropy, "1. Measure of uncertainty/lack of predictability.
2. Expected number of bits needed to losslessly encode a signal."

Interpretation of KL-divergence, "1. Divergence measure quantifying how far \(q\) is from \(p\).
2. Extra number of expected bits necessary to encode signal from \(p\) with \(q\)."

Interpretation of Expected Mutual Information, "1. Information gain if we swith from a model with independent \(p(x)\) and \(p(y)\) to a model with joint \(p(x, y)\).
2. Reduction of uncertainty in \(X\) after observing \(Y\) and vice versa."

Difference between contextual bandit and RL, "In RL; an action in a given state influences the reward as well as the future state. In CB; the action only influeces the reward; not the future state."

Gram Matrix, "\(A^TA\), always positive semidefinite"
Orthogonal matrix \(U\), \(U^T U = U U^T = I\)
Eigenvalue decomposition, "Of a real, symmetrix matrix \(A\): \(A = U \Lambda U^T\), where \(U\) is orthonormal"

Singular value decomposision, "Of a real, \(m \times n\) matrix \(A\):
\(A = USV^T\), where
\(U: m \times m\) has orthonormal columns s.t. \(U^TU = I_m\)
\(V: n \times n\) has orthonormal columns and rows, s.t. \(V^TV = VV^T = I_n\)
\(S: m \times n\) has \(r = min(m, n)\) sigular values on the diagonal
"

Rank-nullity theorem, "For \(A: m \times n\), \(dim(range(A)) + dim(nullspace(A)) = r + (n - r) = n\)
LU factorization, "Any sqaure matrix can be factorized into a product of a lower triangular and upper trinagular matrix."

Partial derivative of \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) wrt \(x_i\), " \(\frac{\partial f}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x + h e_i) - f(x)}{h}\)"

Gradient of \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) wrt point \(x\), "\(\nabla f = \frac{\partial f}{\partial x} = (\frac{\partial f}{\partial x_i}, \dots, \frac{\partial f}{\partial x_n})^T\)
Note that \(\nabla f: \mathbb{R}^n \rightarrow \mathbb{R}^n\) is a vector field."

Jacobian matrix, "Of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\) is an \(m \times n\) matrix of partial derivatives:
\(J_f(x) = \frac{\partial f}{\partial x^T} =  \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n}\\
\dots & \dots & \dots \\
\frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}\)"

Hessian matrix, "Of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is an \(n \times n\) matrix of partial derivatives:
\(H_f(x) = \frac{\partial^2 f}{\partial x^2} = \nabla^2 f = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
\dots & \dots & \dots \\
\frac{\partial f^2}{\partial x_n \partial x_1} & \dots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}\)"



