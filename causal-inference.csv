Propensity score, \(\Pr[T|X]\)
Transformed outcome, \(Y_i^{TO} = T_iY_i - (1-T_i)Y_i\) with \(\mathbb{E}[T_i^{TO}] = \tau_i\)
Average Treatment Effect on Treated, \(\mathbb{E}[Y_1 - Y_0|T=1]\)
Treatment effect (Pearl), \(\mathbb{E}[Y|do(T=1)] - \mathbb{E}[Y|do(T=0)]\)
Bias, \(\mathbb{E}[Y_0|T=1] - \mathbb{E}[Y_0|T=0]\)
Randomized experiment, "\((Y_0, Y_1) \perp T \) - Note the difference from \(Y \perp T\)"

Confounding bias, "\( X\  \rightarrow\ Y\)
\(X \rightarrow T\)
\(T \rightarrow Y\)"

Selection bias, "\(T\) and \(Y\) marginally independent but dependent once conditioned on \(X\). \(X\) can be a collider (1.) or a mediator (2.).
1: \( T\  \rightarrow\ Y, \ T\ \rightarrow\ X, \ Y\ \rightarrow\ X\)
2: \( T\  \rightarrow\ X, \ T\ \rightarrow\ Y, \ X\ \rightarrow\ Y\)
"
Positivity assumption, \(\Pr[T|X] > 0 \)
Difference in Differences estimate, \(\hat{ATET} = (\mathbb{E}[Y(1)|T=1] - \mathbb{E}[Y(1)|T=0]) - (\mathbb{E}[Y(0)|T=1] - \mathbb{E}[Y(0)|T=0]) \)

Fixed Effects Model, "\(y_{it} = \beta_0 + \beta_1 X_{it} + \beta_2 U_i + e_{it} \)
where \(U_i\) are the unobserved fixed effects for unit \(i\)."

Two-way Fixed Effect Model (TWFE), \(Y_{it} = \alpha + \mu_i + \beta_t + \tau D_{it} + \epsilon_{it}\)
Synthetic Control, "Assuming unit \(1\) is treated and \(2 \dots J\) are not, the control group outcome becomes \(\hat{Y_{1t}^N} = \sum_{j=2}^J w_j Y_{jt}\) with optional constraints on \(w\)."
Berkson's paradox, Conditioning on a variable that two variables of interest predict; collider bias; explaining away phenomenon.

Regression discontinuity design, "\(\mathbb{E}[Y_i(1) - Y_i(0)|R_i = r] = \lim_{c \rightarrow r^+} \mathbb{E}[Y_i | R_i=c] - \lim_{c \rightarrow r^-} \mathbb{E}[Y_i | R_i=c] \)
\( y_i = \beta_0 + \beta_1 r_i + \beta_2I[r_i > c] + \beta_3I[r_i > c]r_i\)
where treatment effect is \(\beta_2\) if data center to have \(c=0\)."

F-Learner, "Arbitrary ML model optimizing \(\mathbb{E}[(Y_i^* - \hat{\tau}(X_i))^2]\)
where \(Y_i^* = Y_i T_i 2 - Y_i(1-T_i)2\)"

S-Learner, "Arbitrary ML model regressing \(\mathbb{E}[Y|T, X]\) resulting in treatment estimate \(\mathbb{E}[Y|T=1,X] - \mathbb{E}[Y|T=0,X]\)"
T-Learner, "Two arbitrary ML models regressing \(\mathbb{E}[Y|X]\) subselected to be trained on treatment or control data.
Treatment estimate is given by \(\mu_T(X_i) - \mu_C(X_i)\)"

X-Learner, "\(\hat{M_0} \approx \mathbb{E}[Y|X_{T=0}]\)
\(\hat{M_1} \approx \mathbb{E}[Y|X_{T=1}]\)
\(\hat{M^{\tau_0}} \approx \mathbb{E}[\hat{M_1}(X_i) - Y_i|X_i, T=0]\)
\(\hat{M^{\tau_1}} \approx \mathbb{E}[Y_i - \hat{M_0}(X_i)|X_i, T=1]\)
\( \hat{\tau}(X_i) = \hat{M_{\tau_0}}(X_i) \Pr[T|X] + \hat{M_{\tau_1}}(X_i) (1-\Pr[T|X])\)"

R-Loss, \(\mathcal{L}(\tau(X)) = \frac{1}{n}\sum_{i \in [n]} \tilde{T_i}^2 (\frac{\tilde{Y_i}}{\tilde{T_i}} - \tau(X_i))^2 \) where \(\tilde{Y_i} = Y_i - M_Y(X_i)\) and \(\tilde{T_i} = T_i - M_T(X_i)\)
Synthetic Difference in Differences, "\(\hat{\tau}, \hat{\alpha}, \hat{\mu}, \hat{\beta} = argmin_{\tau, \alpha, \mu, \beta} \sum_{i \in [n]} \sum_{t \in [T]} (Y_{it} - (\alpha + \mu_i + \beta_t + \tau D_{it}))^2 \hat{w_i} \hat{\lambda_t}\)"
Propensity score debiasing, "Resample units with replacements and weights \(W_i = \frac{T_i}{Pr[T|X]} + \frac{1-T_i}{1-Pr[T|X_i]} \)
Can be extended to non-binary case."

Internal validity of a study, Ability of a study to to estimate causal effects within the study population.
External validity of a study, "Generalization of causal inference - extent to which a causal relationship holds over variation in persons, settings, treatments and outcomes."

Assignment mechanism, "\( p: \{0, 1\}^N \times \mathbb{Y}^{2N} \times \mathbb{X}^N \rightarrow [0, 1] \)
Where \(p\) is the probability of the assignment vector \(W\) as a function of outcomes and covariates.
Other fields might refer to this as 'measurement plan' or 'allocation'."

Stratified Randomized Experiment, "\(p(W|Y(0), Y(1), X) = \prod_{g \in G} {N_g \choose N_{t, g} }^{-1} \)
such that \(\forall g \sum_{i=1}^N G_{ig} W_i = N_{t, g}\)
where \(G\) is a partitioning of the covariate space \(\mathbb{X}\).
In other words, we have \(N_{t, g}\) treated units in stratum \(g\).
Note: \(X\) and \(W\) represent information for all units."

Paired Randomized Expirement, "\(p(W|Y(0), Y(1), X) = \frac{1}{2}^{N/2}\) for all \(W\)
s.t. \(\forall g \sum_{i=1}^N G_{ig} W_i = 1\)
In other words, we have exactly one treated unit per pair."

Spillover effect, Interactions between units due to absence or presence of treatment.

Clustered Randomized Expirement, "\(p(W|Y(0), Y(1), X) = {G \choose G_t}^{-1} \) for all \(W\)
s.t. \( G_{ig} = G_{i'g} = 1 \Rightarrow W_i = W_{i'}\) and \(\sum_{g=1}^G \bar{W_g} = G_t\)
In other words, every unit per cluster receives the same assignment and in total \(G_t\) clusters are selected to be treated."

\(s\)-th quantile treatement effect, "\(\tau_s = q_{Y(1)}(s) - q_{Y(0)}(s)\)
Where \(q_Y(s) = \inf_y I[F_Y(y) \leq s]\)"

Power calculation, "\(f: [0, 1] \times \mathbb{R} \rightarrow \mathbb{N}^+
(p, \tau) \mapsto n\)
where \(p=\Pr[reject\ H_0|H_0\ is\ false]\) is the power and \(n\) is minimal amount of samples required"

Motivation for clustered randomized experiment, "1. Account for interactions between treated units.
2. It might be simpler to sample at cluster level."

One-sided non-compliance, "Some units assigned to treatment might not take it. Units assigned to control comply and will not take the treatment."
Two-sided non-compliance, "Any unit might disregard their assigment."

Approaches to deal with non-compliance, "1. Ignore receipt and tofucs on TE of assignment ('Intention-to-treat analysis')
2. Instrumental Variable; causal effect of receipt for subpopulation of compliers
3. 'Bounds analysis'
4. 'As-treated', pretend people complied
5. Drop non-compliers"

Honest tree, "Splits learned from one sample, CATEs in leafs trained on another sample. Avoids bias."

Prognostic effect, "\(\mathbb{E}[Y|X, T=0]\)"

BART priors, "Tree structure:
1. Given number of trees \(L\), splitting probability is a depth \(h\) is \(\frac{\eta}{(1 + h)^{\beta}}\)
2. Uniform distribution among features to use for split
3. Uniform distribution over feature-values to split at
Leaf values: \(m_{lb} \approx \mathcal{N}(0, \sigma_m^2)\)"